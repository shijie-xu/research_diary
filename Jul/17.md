#### [Deep Learning with Differential Privacy](https://arxiv.org/pdf/1607.00133)

I think this is an interesting paper about differential privacy I have read, which shows how we can use differential privacy mechanism(Gaussian mechanism) in the deep learning field.

Deep learning with DP in this paper contains 3 parts: SGD algorithm, moments accountant and hyperparameter tuning. SGD algorithm show how the privacy mechanism works in the stochastic gradient descent algorithm, and the moments accountant part keeps track of a bound of loss random variable.

Most of the techniques has been proposed in the previous work. But a tighter bound is given in this paper, which is stronger than the strong composition theorem, in fact, the authors give an special composition theorem with this tighter bound in the paper. This theorem is the main contribution of this paper.

***

following techniques also has been mentioned in the [last paper](16.md), so what are them?

> in particular, **regularization techniques**, which aim to avoid overfitting to the examples used for training, may hide details of those examples

> ...for example, Fredrikson et. al. demonstrated a **model-inversion attack** that recovers images from a facial recognition system



I need examples to explain this precisely:

> **Group privacy** implies graceful degradation of privacy guarantees if datasets contain correlated inputs, such as the ones contributed by the same individual. Robustness to auxiliary information means that privacy guarantees are not affected by any side information available to the adversary
